\documentclass[11pt,letterpaper]{article}
\usepackage[lmargin=1in,rmargin=1in,bmargin=1in,tmargin=1in]{geometry}
\usepackage{checkins}

% -------------------
% Content
% -------------------
\begin{document}
\thispagestyle{title}

% 01/14
\checkin{01/14} The vector $\mathbf{u}= \langle 2, -1, 0 \rangle$ is a unit vector. \pspace

\sol The statement is \textit{false}. We know a unit vector has length 1. We know if $\mathbf{v}= \langle x_1, x_2, \ldots, x_n \rangle$, then $\|\mathbf{v}\|= \sqrt{x_1^2 + x_2^2 + \cdots + x_n^2}$. But then $\|\mathbf{u}\|= \| \langle 2, -1, 0 \rangle= \sqrt{2^2 + (-1)^2 + 0^2}= \sqrt{4 + 1 + 0}= \sqrt{5} \neq 1$. Therefore, $\mathbf{u}$ is not a unit vector. \pvspace{1.3cm}


% 01/16
\checkin{01/16} Suppose that $\mathbf{u}, \mathbf{v} \in \mathbb{R}^n$. If $\mathbf{u} \cdot \mathbf{v}= 0$, then either $\mathbf{u}= 0$ or $\mathbf{v}= 0$. Furthermore, $\mathbf{u} \perp \mathbf{v}$. \pspace

\sol The statement is \textit{false}. If $\mathbf{u} \cdot \mathbf{v}= 0$, it is true that $\mathbf{u}$ and $\mathbf{v}$ are perpendicular (so long as neither of them are nonzero). Furthermore, if $\mathbf{u}$ and $\mathbf{v}$ are perpendicular, then $\mathbf{u} \cdot \mathbf{v}= 0$. However, if $\mathbf{u} \cdot \mathbf{v}= 0$, it need not be the case that $\mathbf{u}$ and $\mathbf{v}$ are zero. For instance, if $\mathbf{u}= \langle 1, 0, \ldots, 0 \rangle$ and $\mathbf{v}= \langle 0, 1, 0, \ldots, 0 \rangle$, then $\mathbf{u} \cdot \mathbf{v}= 1(0) + 0(1) + 0(0) + \cdots + 0(0)= 0$ but neither $\mathbf{u}$ nor $\mathbf{v}$ are zero. Furthermore, it is impossible that $\mathbf{u}= 0$ or $\mathbf{v}= 0$. Both $\mathbf{u}, \mathbf{v}$ are \textit{vectors} while 0 is a scalar. \pvspace{1.3cm}



% 01/21
\checkin{01/21} The augmented matrices $\begin{pmatrix*}[r] 1 & 2 & 3 & 0 \\ -1 & 5 & 1 & 5 \\ 1 & -1 & 1 & -2 \end{pmatrix*}$ and $\begin{pmatrix*}[r] -1 & 5 & 1 & 5 \\ 1 & 2 & 3 & 0 \\ 2 & -2 & 2 & -4 \end{pmatrix*}$ represent equivalent systems of linear equations. \pspace

\sol The statement is \textit{true}. We know that for systems of linear equations, we can represent these systems as augmented matrices. Elementary row operations on these matrices correspond to viable arithmetic operations on the system of equation `side.' These operations preserve the solutions (if any) to the system of equations, i.e. produce equivalent systems. So, we can perform any elementary row operation on these augmented matrices and obtain equivalent systems: interchange rows, scale a row by a nonzero scalar, and replace any single row with a linear combination of other rows. Observe we can obtain the second matrix from the first by interchanging the first two rows and scaling the third row by 2. Therefore, the systems of linear equations represented by these matrices are equivalent. If the first matrix was an augmented matrix that was `untouched' from the original system. The system of equations was\dots
	\[
	\begin{cases}
	x + 2y + 3z= 0 \\
	-x + 5y + z= 5 \\
	x - y + z= -2 
	\end{cases}
	\]
which has solution $(-\frac{1}{2}, 1, -\frac{1}{2})$. \pvspace{1.3cm}



% 01/23
\checkin{01/23} There exists a linear system consisting of some number of equations with some number of variables such that there are five solutions to the given system. \pspace

\sol The statement is \textit{false}. For a system of linear equations, there are only three possibilities: there are no solutions to the system of equations, there is \textit{one} solution to the system of equations, or there are infinitely many solutions. It is not possible to have any other number of solutions. \pvspace{1.3cm}



% 01/26
\checkin{01/26} If a system of linear equations has a unique solution, then the RREF of the augmented matrix has a pivot position in every column except for the last one---which corresponds to the solution. \pspace 

\sol The statement is \textit{true}. Denote the coefficient matrix by $A$. If the augmented matrix has a pivot position in every column except for the last one, then $A$ has a pivot position in every column. We know each column corresponds to variable in the original system of equations. We also know that each pivot column corresponds to a basic variable, i.e. one whose value is either fixed or given in terms of a free variable. But because every column has a pivot position, we know each of the variables in the system are basic variables, i.e. there are no free variables. Therefore, each of the variables has a fixed value, i.e. there is a unique solution. \pvspace{1.3cm}



% 01/28
\checkin{01/28} Consider a matrix equation $A \mathbf{x}= \mathbf{b}$. If the RREF of $A$ has no zero rows, then the matrix-vector equation has a solution. \pspace

\sol The statement is \textit{true}. If the RREF of $A$ has no zero rows, there can be no row in the RREF augmented matrix $[A \;\;\mathbf{b}]$ can have no row of the form $[0 \;\;0\;\; \cdots\;\;0\;\;1]$. But then the original system cannot be inconsistent, i.e. it is consistent. Therefore, there must be at least one (possibly infinitely many) solutions to the original system of equations. \pvspace{1.3cm}



% 01/30
\checkin{01/30} If REF of an augmented matrix coming from a system of linear equations is $\begin{pmatrix} 1 & 1 & 1 & -1 & 0 \\ 0 & 1 & -2 & 1 & 4 \\ 0 & 0 & 0 & 2 & 6 \end{pmatrix}$, then the solution is $(x_1, x_2, x_3, x_4)= (2 - 3t, 2t + 1, t, 3)$, where $t$ is any real number. \pspace

\sol The statement is \textit{true}. From the last row, we know that $2x_4= 6$, i.e. $x_4= 3$. From the second row, we know that $x_2 - 2x_3 + x_4= 4$. But we know $x_4= 3$. Therefore, $x_2 - 2x_3 + 3= 4$, which implies $x_2= 2x_3 + 1$. Finally, from the first row, we know $x_1 + x_2 + x_3 - x_4= 0$. Using the fact that $x_4= 3$ and $x_2= 2x_3 + 1$, we know $x_1 + (2x_3 + 1) + x_3 - 3= 0$, which implies $x_1= 2 - 3x_3$. Letting $x_3= t$, we have $(x_1, x_2, x_3, x_4)= (2 - 3t, 2t + 1, t, 3)$. \pvspace{1.3cm}



% 02/04
\checkin{02/04} If a $10 \times 10$ matrix $A$ has rank 7, then it cannot be invertible. Furthermore, there must be a vector $\mathbf{b} \in \mathbb{R}^{10}$ such that $A\mathbf{x}= \mathbf{b}$ has no solution. \pspace

\sol The statement is \textit{true}. We know that $A$ has an inverse if there is a matrix $B$ such that $AB= I$, where $I$ is the identity matrix. This occurs if and only if we can form an augmented matrix $[A \;\;I]$ and place $A$ into RREF and obtain an identity matrix. If so, then $A^{-1}$ appears in the columns where $I$ was in this augmented matrix. If $A$ has rank 7, then has only 7 pivot positions in its RREF. But then the RREF of $A$ cannot be the identity matrix, which in this case would have 10 pivot positions. Therefore, $A$ is not invertible. Moreover, because $A$ has only 7 pivot positions in its RREF, there must be at least one row of zeros in its RREF. But then we can find a $\mathbf{b}$ vector such that, after these row reductions, we have a row in the RREF of the augmented matrix $[A\;\;\mathbf{b}]$ of the form $[0 \;\;0\;\; \cdots\;\;0\;\;1]$. But then the system $A\mathbf{x}= \mathbf{b}$ has no solution. \pvspace{1.3cm}



% 02/06
\checkin{02/06} $\det \begin{pmatrix} a & b \\ c & d \end{pmatrix}= ad - bc$ \pspace

\sol The statement is \textit{true}. Indeed, this is how we define the determinant for a two-by-two matrix. We then define the determinant of larger $n \times n$ matrices via cofactor expansions, which eventually reduces the problem of finding the given determinant into computing determinants of various $2 \times 2$ matrices. \pvspace{1.3cm}



% 02/09
\checkin{02/09} If $A,B$ are $n \times n$ matrices, $B$ is invertible, and $\det(AB)= 0$, then it \textit{must be} that $A$ is not invertible. \pspace

\sol The statement is \textit{true}. We know that a square matrix $A$ is invertible if and only if $\det A \neq 0$. We also know that $\det(AB)= \det A \det B$. Because $0= \det(AB)= \det A \det B$, either $\det A= 0$ or $\det B= 0$. Because $B$ is invertible, we know $\det B \neq 0$. But then it must be that $\det A= 0$, which implies that $A$ is not invertible. \pvspace{1.3cm}



% 02/18
\checkin{02/18} An $LU$ factorization for a matrix $A$ is a decomposition of $A$ into a product of the form $A= LU$, where $L$ is lower triangular and $U$ is upper triangular. \pspace

\sol The statement is \textit{true}. This is the definition of an $LU$ factorization/decomposition. In fact, one typically also requires (if possible) the matrix $L$ to be unit lower triangular, i.e. the diagonal entries of $L$ to be equal to 1. \pvspace{1.3cm}



% 02/20
\checkin{02/20} \textit{Every} $n \times m$ matrix $A$ has a $PLU$ factorization. \pspace

\sol The statement is \textit{true}. Computing an LU decomposition is essentially computing the row echelon form of a matrix without scaling or row interchanges. Of course, this is not always possible, so not all matrices have an LU factorization. But if one allows row interchanges, one can always find the REF of a matrix. But then one should always be able to find a PLU factorization for a matrix. \pvspace{1.3cm}














\end{document}